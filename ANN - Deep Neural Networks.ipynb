{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice training a deep neural network on the CIFAR10 image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearing the session removes all the nodes left over from previous models, freeing memory and preventing slowdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the output layer to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nadam optimizer with a learning rate of 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading CIFAR10 dataset. We also want to use early stopping, so we need a validation set. Let's use the first 5,000 images of the original training set as the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[158, 112,  49],\n",
       "         [159, 111,  47],\n",
       "         [165, 116,  51],\n",
       "         ...,\n",
       "         [137,  95,  36],\n",
       "         [126,  91,  36],\n",
       "         [116,  85,  33]],\n",
       "\n",
       "        [[152, 112,  51],\n",
       "         [151, 110,  40],\n",
       "         [159, 114,  45],\n",
       "         ...,\n",
       "         [136,  95,  31],\n",
       "         [125,  91,  32],\n",
       "         [119,  88,  34]],\n",
       "\n",
       "        [[151, 110,  47],\n",
       "         [151, 109,  33],\n",
       "         [158, 111,  36],\n",
       "         ...,\n",
       "         [139,  98,  34],\n",
       "         [130,  95,  34],\n",
       "         [120,  89,  33]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 68, 124, 177],\n",
       "         [ 42, 100, 148],\n",
       "         [ 31,  88, 137],\n",
       "         ...,\n",
       "         [ 38,  97, 146],\n",
       "         [ 13,  64, 108],\n",
       "         [ 40,  85, 127]],\n",
       "\n",
       "        [[ 61, 116, 168],\n",
       "         [ 49, 102, 148],\n",
       "         [ 35,  85, 132],\n",
       "         ...,\n",
       "         [ 26,  82, 130],\n",
       "         [ 29,  82, 126],\n",
       "         [ 20,  64, 107]],\n",
       "\n",
       "        [[ 54, 107, 160],\n",
       "         [ 56, 105, 149],\n",
       "         [ 45,  89, 132],\n",
       "         ...,\n",
       "         [ 24,  77, 124],\n",
       "         [ 34,  84, 129],\n",
       "         [ 21,  67, 110]]],\n",
       "\n",
       "\n",
       "       [[[235, 235, 235],\n",
       "         [231, 231, 231],\n",
       "         [232, 232, 232],\n",
       "         ...,\n",
       "         [233, 233, 233],\n",
       "         [233, 233, 233],\n",
       "         [232, 232, 232]],\n",
       "\n",
       "        [[238, 238, 238],\n",
       "         [235, 235, 235],\n",
       "         [235, 235, 235],\n",
       "         ...,\n",
       "         [236, 236, 236],\n",
       "         [236, 236, 236],\n",
       "         [235, 235, 235]],\n",
       "\n",
       "        [[237, 237, 237],\n",
       "         [234, 234, 234],\n",
       "         [234, 234, 234],\n",
       "         ...,\n",
       "         [235, 235, 235],\n",
       "         [235, 235, 235],\n",
       "         [234, 234, 234]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 87,  99,  89],\n",
       "         [ 43,  51,  37],\n",
       "         [ 19,  23,  11],\n",
       "         ...,\n",
       "         [169, 184, 179],\n",
       "         [182, 197, 193],\n",
       "         [188, 202, 201]],\n",
       "\n",
       "        [[ 82,  96,  82],\n",
       "         [ 46,  57,  36],\n",
       "         [ 36,  44,  22],\n",
       "         ...,\n",
       "         [174, 189, 183],\n",
       "         [185, 200, 196],\n",
       "         [187, 202, 200]],\n",
       "\n",
       "        [[ 85, 101,  83],\n",
       "         [ 62,  75,  48],\n",
       "         [ 58,  67,  38],\n",
       "         ...,\n",
       "         [168, 183, 178],\n",
       "         [180, 195, 191],\n",
       "         [186, 200, 199]]],\n",
       "\n",
       "\n",
       "       [[[158, 190, 222],\n",
       "         [158, 187, 218],\n",
       "         [139, 166, 194],\n",
       "         ...,\n",
       "         [228, 231, 234],\n",
       "         [237, 239, 243],\n",
       "         [238, 241, 246]],\n",
       "\n",
       "        [[170, 200, 229],\n",
       "         [172, 199, 226],\n",
       "         [151, 176, 201],\n",
       "         ...,\n",
       "         [232, 232, 236],\n",
       "         [246, 246, 250],\n",
       "         [246, 247, 251]],\n",
       "\n",
       "        [[174, 201, 225],\n",
       "         [176, 200, 222],\n",
       "         [157, 179, 199],\n",
       "         ...,\n",
       "         [230, 229, 232],\n",
       "         [250, 249, 251],\n",
       "         [245, 244, 247]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 31,  40,  45],\n",
       "         [ 30,  39,  44],\n",
       "         [ 26,  35,  40],\n",
       "         ...,\n",
       "         [ 37,  40,  46],\n",
       "         [  9,  13,  14],\n",
       "         [  4,   7,   5]],\n",
       "\n",
       "        [[ 23,  34,  39],\n",
       "         [ 27,  38,  43],\n",
       "         [ 25,  36,  41],\n",
       "         ...,\n",
       "         [ 19,  20,  24],\n",
       "         [  4,   6,   3],\n",
       "         [  5,   7,   3]],\n",
       "\n",
       "        [[ 28,  41,  47],\n",
       "         [ 30,  43,  50],\n",
       "         [ 32,  45,  52],\n",
       "         ...,\n",
       "         [  5,   6,   8],\n",
       "         [  4,   5,   3],\n",
       "         [  7,   8,   7]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 20,  15,  12],\n",
       "         [ 19,  14,  11],\n",
       "         [ 15,  14,  11],\n",
       "         ...,\n",
       "         [ 10,   9,   7],\n",
       "         [ 12,  11,   9],\n",
       "         [ 13,  12,  10]],\n",
       "\n",
       "        [[ 21,  16,  13],\n",
       "         [ 20,  16,  13],\n",
       "         [ 18,  17,  12],\n",
       "         ...,\n",
       "         [ 10,   9,   7],\n",
       "         [ 10,   9,   7],\n",
       "         [ 12,  11,   9]],\n",
       "\n",
       "        [[ 21,  16,  13],\n",
       "         [ 21,  17,  12],\n",
       "         [ 20,  18,  11],\n",
       "         ...,\n",
       "         [ 12,  11,   9],\n",
       "         [ 12,  11,   9],\n",
       "         [ 13,  12,  10]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 33,  25,  13],\n",
       "         [ 34,  26,  15],\n",
       "         [ 34,  26,  15],\n",
       "         ...,\n",
       "         [ 28,  25,  52],\n",
       "         [ 29,  25,  58],\n",
       "         [ 23,  20,  42]],\n",
       "\n",
       "        [[ 33,  25,  14],\n",
       "         [ 34,  26,  15],\n",
       "         [ 34,  26,  15],\n",
       "         ...,\n",
       "         [ 27,  24,  52],\n",
       "         [ 27,  24,  56],\n",
       "         [ 25,  22,  47]],\n",
       "\n",
       "        [[ 31,  23,  12],\n",
       "         [ 32,  24,  13],\n",
       "         [ 33,  25,  14],\n",
       "         ...,\n",
       "         [ 24,  23,  50],\n",
       "         [ 26,  23,  53],\n",
       "         [ 25,  20,  47]]],\n",
       "\n",
       "\n",
       "       [[[ 25,  40,  12],\n",
       "         [ 15,  36,   3],\n",
       "         [ 23,  41,  18],\n",
       "         ...,\n",
       "         [ 61,  82,  78],\n",
       "         [ 92, 113, 112],\n",
       "         [ 75,  89,  92]],\n",
       "\n",
       "        [[ 12,  25,   6],\n",
       "         [ 20,  37,   7],\n",
       "         [ 24,  36,  15],\n",
       "         ...,\n",
       "         [115, 134, 138],\n",
       "         [149, 168, 177],\n",
       "         [104, 117, 131]],\n",
       "\n",
       "        [[ 12,  25,  11],\n",
       "         [ 15,  29,   6],\n",
       "         [ 34,  40,  24],\n",
       "         ...,\n",
       "         [154, 172, 182],\n",
       "         [157, 175, 192],\n",
       "         [116, 129, 151]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[100, 129,  81],\n",
       "         [103, 132,  84],\n",
       "         [104, 134,  86],\n",
       "         ...,\n",
       "         [ 97, 128,  84],\n",
       "         [ 98, 126,  84],\n",
       "         [ 91, 121,  79]],\n",
       "\n",
       "        [[103, 132,  83],\n",
       "         [104, 131,  83],\n",
       "         [107, 135,  87],\n",
       "         ...,\n",
       "         [101, 132,  87],\n",
       "         [ 99, 127,  84],\n",
       "         [ 92, 121,  79]],\n",
       "\n",
       "        [[ 95, 126,  78],\n",
       "         [ 95, 123,  76],\n",
       "         [101, 128,  81],\n",
       "         ...,\n",
       "         [ 93, 124,  80],\n",
       "         [ 95, 123,  81],\n",
       "         [ 92, 120,  80]]],\n",
       "\n",
       "\n",
       "       [[[ 73,  78,  75],\n",
       "         [ 98, 103, 113],\n",
       "         [ 99, 106, 114],\n",
       "         ...,\n",
       "         [135, 150, 152],\n",
       "         [135, 149, 154],\n",
       "         [203, 215, 223]],\n",
       "\n",
       "        [[ 69,  73,  70],\n",
       "         [ 84,  89,  97],\n",
       "         [ 68,  75,  81],\n",
       "         ...,\n",
       "         [ 85,  95,  89],\n",
       "         [ 71,  82,  80],\n",
       "         [120, 133, 135]],\n",
       "\n",
       "        [[ 69,  73,  70],\n",
       "         [ 90,  95, 100],\n",
       "         [ 62,  71,  74],\n",
       "         ...,\n",
       "         [ 74,  81,  70],\n",
       "         [ 53,  62,  54],\n",
       "         [ 62,  74,  69]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[123, 128,  96],\n",
       "         [132, 132, 102],\n",
       "         [129, 128, 100],\n",
       "         ...,\n",
       "         [108, 107,  88],\n",
       "         [ 62,  60,  55],\n",
       "         [ 27,  27,  28]],\n",
       "\n",
       "        [[115, 121,  91],\n",
       "         [123, 124,  95],\n",
       "         [129, 126,  99],\n",
       "         ...,\n",
       "         [115, 116,  94],\n",
       "         [ 66,  65,  59],\n",
       "         [ 27,  27,  27]],\n",
       "\n",
       "        [[116, 120,  90],\n",
       "         [121, 122,  94],\n",
       "         [129, 128, 101],\n",
       "         ...,\n",
       "         [116, 115,  94],\n",
       "         [ 68,  65,  58],\n",
       "         [ 27,  26,  26]]]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 32, 32, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the callbacks and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### %tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   1/1407 [..............................] - ETA: 0s - loss: 165.9660 - accuracy: 0.0625WARNING:tensorflow:From /Users/paulmospan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/1407 [..............................] - ETA: 1:07 - loss: 133.4807 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.0888s). Check your callbacks.\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 4.1571 - accuracy: 0.1532 - val_loss: 2.1801 - val_accuracy: 0.2046\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 2.0763 - accuracy: 0.2407 - val_loss: 2.0508 - val_accuracy: 0.2356\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.9539 - accuracy: 0.2802 - val_loss: 1.9222 - val_accuracy: 0.2888\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.8691 - accuracy: 0.3164 - val_loss: 1.9374 - val_accuracy: 0.3124\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.8007 - accuracy: 0.3422 - val_loss: 1.7948 - val_accuracy: 0.3378\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7477 - accuracy: 0.3640 - val_loss: 1.7453 - val_accuracy: 0.3648\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7037 - accuracy: 0.3812 - val_loss: 1.7323 - val_accuracy: 0.3700\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6687 - accuracy: 0.3957 - val_loss: 1.6907 - val_accuracy: 0.3884\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6386 - accuracy: 0.4057 - val_loss: 1.6505 - val_accuracy: 0.3980\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6106 - accuracy: 0.4174 - val_loss: 1.6860 - val_accuracy: 0.3828\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5898 - accuracy: 0.4262 - val_loss: 1.6219 - val_accuracy: 0.4162\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5683 - accuracy: 0.4341 - val_loss: 1.6268 - val_accuracy: 0.4138\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5460 - accuracy: 0.4424 - val_loss: 1.6344 - val_accuracy: 0.4018\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5302 - accuracy: 0.4478 - val_loss: 1.6082 - val_accuracy: 0.4252\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.5121 - accuracy: 0.4548 - val_loss: 1.5981 - val_accuracy: 0.4204\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4945 - accuracy: 0.4618 - val_loss: 1.5902 - val_accuracy: 0.4332\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4832 - accuracy: 0.4666 - val_loss: 1.5964 - val_accuracy: 0.4338\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4693 - accuracy: 0.4714 - val_loss: 1.5692 - val_accuracy: 0.4386\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4548 - accuracy: 0.4753 - val_loss: 1.5734 - val_accuracy: 0.4366\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4451 - accuracy: 0.4780 - val_loss: 1.5719 - val_accuracy: 0.4446\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4312 - accuracy: 0.4850 - val_loss: 1.5846 - val_accuracy: 0.4388\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4203 - accuracy: 0.4868 - val_loss: 1.5399 - val_accuracy: 0.4508\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4089 - accuracy: 0.4933 - val_loss: 1.5617 - val_accuracy: 0.4444\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3972 - accuracy: 0.4963 - val_loss: 1.5635 - val_accuracy: 0.4434\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3872 - accuracy: 0.4986 - val_loss: 1.5325 - val_accuracy: 0.4528\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3765 - accuracy: 0.5038 - val_loss: 1.5792 - val_accuracy: 0.4398\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3668 - accuracy: 0.5070 - val_loss: 1.5321 - val_accuracy: 0.4648\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3563 - accuracy: 0.5124 - val_loss: 1.5484 - val_accuracy: 0.4454\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3513 - accuracy: 0.5136 - val_loss: 1.5252 - val_accuracy: 0.4586\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3387 - accuracy: 0.5185 - val_loss: 1.5789 - val_accuracy: 0.4428\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3300 - accuracy: 0.5209 - val_loss: 1.5967 - val_accuracy: 0.4512\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3234 - accuracy: 0.5236 - val_loss: 1.5649 - val_accuracy: 0.4484\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3125 - accuracy: 0.5265 - val_loss: 1.5494 - val_accuracy: 0.4570\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3042 - accuracy: 0.5305 - val_loss: 1.5489 - val_accuracy: 0.4628\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2969 - accuracy: 0.5325 - val_loss: 1.5543 - val_accuracy: 0.4580\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2933 - accuracy: 0.5357 - val_loss: 1.5541 - val_accuracy: 0.4556\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2809 - accuracy: 0.5390 - val_loss: 1.5607 - val_accuracy: 0.4600\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2733 - accuracy: 0.5430 - val_loss: 1.5406 - val_accuracy: 0.4668\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2640 - accuracy: 0.5427 - val_loss: 1.5706 - val_accuracy: 0.4438\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2602 - accuracy: 0.5465 - val_loss: 1.5660 - val_accuracy: 0.4508\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2524 - accuracy: 0.5482 - val_loss: 1.5785 - val_accuracy: 0.4520\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2451 - accuracy: 0.5534 - val_loss: 1.5377 - val_accuracy: 0.4662\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2347 - accuracy: 0.5564 - val_loss: 1.5937 - val_accuracy: 0.4486\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2306 - accuracy: 0.5572 - val_loss: 1.5718 - val_accuracy: 0.4624\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2214 - accuracy: 0.5594 - val_loss: 1.5408 - val_accuracy: 0.4646\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2167 - accuracy: 0.5612 - val_loss: 1.5391 - val_accuracy: 0.4670\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2095 - accuracy: 0.5627 - val_loss: 1.5472 - val_accuracy: 0.4664\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2011 - accuracy: 0.5647 - val_loss: 1.5542 - val_accuracy: 0.4658\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1956 - accuracy: 0.5700 - val_loss: 1.5759 - val_accuracy: 0.4604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8cb845d050>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.5252 - accuracy: 0.0942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5251532793045044, 0.094200000166893]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BN layer added after every Dense layer (before the activation function), except for the output layer. Plus one BN layer before the first hidden layer.\n",
    "\n",
    "Learning rate changed to 5e-4.\n",
    "\n",
    "Run directories renamed to runbn* and the model file name to my_cifar10_bn_model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 520,498\n",
      "Trainable params: 510,354\n",
      "Non-trainable params: 10,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1407 [..............................] - ETA: 6:24 - loss: 2.8693 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.5389s). Check your callbacks.\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.8408 - accuracy: 0.3389 - val_loss: 1.7057 - val_accuracy: 0.3910\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6651 - accuracy: 0.4066 - val_loss: 1.5858 - val_accuracy: 0.4318\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5956 - accuracy: 0.4340 - val_loss: 1.5351 - val_accuracy: 0.4366\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5451 - accuracy: 0.4498 - val_loss: 1.5087 - val_accuracy: 0.4670\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5044 - accuracy: 0.4649 - val_loss: 1.4338 - val_accuracy: 0.4872\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.4677 - accuracy: 0.4774 - val_loss: 1.4202 - val_accuracy: 0.4940\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4352 - accuracy: 0.4882 - val_loss: 1.4210 - val_accuracy: 0.4884\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.4046 - accuracy: 0.4995 - val_loss: 1.3865 - val_accuracy: 0.5024\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3834 - accuracy: 0.5111 - val_loss: 1.3671 - val_accuracy: 0.5180\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3616 - accuracy: 0.5171 - val_loss: 1.3684 - val_accuracy: 0.5212\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.3398 - accuracy: 0.5252 - val_loss: 1.3605 - val_accuracy: 0.5244\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3187 - accuracy: 0.5321 - val_loss: 1.3755 - val_accuracy: 0.5066\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2974 - accuracy: 0.5386 - val_loss: 1.3680 - val_accuracy: 0.5136\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2837 - accuracy: 0.5446 - val_loss: 1.3533 - val_accuracy: 0.5210\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2634 - accuracy: 0.5529 - val_loss: 1.3584 - val_accuracy: 0.5228\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2540 - accuracy: 0.5555 - val_loss: 1.3354 - val_accuracy: 0.5282\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2344 - accuracy: 0.5618 - val_loss: 1.3361 - val_accuracy: 0.5290\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2171 - accuracy: 0.5698 - val_loss: 1.3386 - val_accuracy: 0.5344\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2072 - accuracy: 0.5728 - val_loss: 1.3379 - val_accuracy: 0.5394\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1940 - accuracy: 0.5763 - val_loss: 1.3608 - val_accuracy: 0.5316\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1832 - accuracy: 0.5803 - val_loss: 1.3523 - val_accuracy: 0.5350\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1630 - accuracy: 0.5900 - val_loss: 1.3442 - val_accuracy: 0.5276\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1529 - accuracy: 0.5938 - val_loss: 1.3439 - val_accuracy: 0.5316\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1407 - accuracy: 0.5958 - val_loss: 1.3259 - val_accuracy: 0.5444\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1311 - accuracy: 0.6018 - val_loss: 1.3256 - val_accuracy: 0.5448\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1143 - accuracy: 0.6078 - val_loss: 1.3662 - val_accuracy: 0.5282\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0995 - accuracy: 0.6108 - val_loss: 1.3467 - val_accuracy: 0.5416\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0977 - accuracy: 0.6127 - val_loss: 1.3573 - val_accuracy: 0.5342\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0837 - accuracy: 0.6173 - val_loss: 1.3346 - val_accuracy: 0.5410\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0735 - accuracy: 0.6209 - val_loss: 1.3554 - val_accuracy: 0.5378\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0612 - accuracy: 0.6248 - val_loss: 1.3701 - val_accuracy: 0.5332\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0520 - accuracy: 0.6274 - val_loss: 1.3643 - val_accuracy: 0.5398\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0426 - accuracy: 0.6313 - val_loss: 1.3541 - val_accuracy: 0.5398\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0328 - accuracy: 0.6371 - val_loss: 1.3416 - val_accuracy: 0.5490\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0209 - accuracy: 0.6389 - val_loss: 1.3501 - val_accuracy: 0.5482\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0161 - accuracy: 0.6424 - val_loss: 1.3571 - val_accuracy: 0.5448\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 0.9993 - accuracy: 0.6458 - val_loss: 1.3443 - val_accuracy: 0.5450\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9950 - accuracy: 0.6471 - val_loss: 1.3661 - val_accuracy: 0.5460\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 0.9775 - accuracy: 0.6528 - val_loss: 1.3949 - val_accuracy: 0.5382\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9711 - accuracy: 0.6585 - val_loss: 1.4085 - val_accuracy: 0.5284\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9677 - accuracy: 0.6591 - val_loss: 1.3859 - val_accuracy: 0.5430\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9549 - accuracy: 0.6608 - val_loss: 1.3774 - val_accuracy: 0.5418\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9518 - accuracy: 0.6635 - val_loss: 1.4219 - val_accuracy: 0.5382\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 0.9454 - accuracy: 0.6654 - val_loss: 1.4026 - val_accuracy: 0.5310\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.9342 - accuracy: 0.6710 - val_loss: 1.4102 - val_accuracy: 0.5386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8cbed31490>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.3256 - accuracy: 0.0900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3256243467330933, 0.09000000357627869]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Batch Normalization with SELU, and making the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1407 [..............................] - ETA: 3:29 - loss: 3.0440 - accuracy: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0055s vs `on_train_batch_end` time: 0.2920s). Check your callbacks.\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.9349 - accuracy: 0.3018 - val_loss: 1.8360 - val_accuracy: 0.3250\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7235 - accuracy: 0.3900 - val_loss: 1.7700 - val_accuracy: 0.3612\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6253 - accuracy: 0.4247 - val_loss: 1.7223 - val_accuracy: 0.4002\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.5581 - accuracy: 0.4510 - val_loss: 1.6428 - val_accuracy: 0.4280\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.5062 - accuracy: 0.4714 - val_loss: 1.6041 - val_accuracy: 0.4366\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.4547 - accuracy: 0.4899 - val_loss: 1.5172 - val_accuracy: 0.4786\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4115 - accuracy: 0.5046 - val_loss: 1.5435 - val_accuracy: 0.4606\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3714 - accuracy: 0.5190 - val_loss: 1.4979 - val_accuracy: 0.4756\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3375 - accuracy: 0.5352 - val_loss: 1.5027 - val_accuracy: 0.4756\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3097 - accuracy: 0.5461 - val_loss: 1.4890 - val_accuracy: 0.4910\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2814 - accuracy: 0.5550 - val_loss: 1.4899 - val_accuracy: 0.4938\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2538 - accuracy: 0.5674 - val_loss: 1.5147 - val_accuracy: 0.4828\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.2265 - accuracy: 0.5739 - val_loss: 1.5015 - val_accuracy: 0.5016\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1965 - accuracy: 0.5867 - val_loss: 1.4646 - val_accuracy: 0.5094\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 1.1731 - accuracy: 0.5938 - val_loss: 1.5334 - val_accuracy: 0.4852\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1589 - accuracy: 0.5999 - val_loss: 1.5475 - val_accuracy: 0.4934\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1385 - accuracy: 0.6098 - val_loss: 1.4751 - val_accuracy: 0.4964\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1126 - accuracy: 0.6172 - val_loss: 1.5222 - val_accuracy: 0.4914\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1408 - accuracy: 0.6072 - val_loss: 1.5443 - val_accuracy: 0.5020\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 2.0066 - accuracy: 0.5883 - val_loss: 1.5685 - val_accuracy: 0.4676\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2052 - accuracy: 0.5764 - val_loss: 1.5180 - val_accuracy: 0.4984\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1304 - accuracy: 0.6080 - val_loss: 1.5624 - val_accuracy: 0.5004\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2124 - accuracy: 0.5942 - val_loss: 1.5265 - val_accuracy: 0.4896\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0965 - accuracy: 0.6202 - val_loss: 1.5565 - val_accuracy: 0.5024\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.1231 - accuracy: 0.6073 - val_loss: 1.5565 - val_accuracy: 0.4988\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0514 - accuracy: 0.6357 - val_loss: 1.5668 - val_accuracy: 0.4924\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0182 - accuracy: 0.6497 - val_loss: 1.5595 - val_accuracy: 0.4984\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0088 - accuracy: 0.6547 - val_loss: 1.5415 - val_accuracy: 0.4994\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0001 - accuracy: 0.6574 - val_loss: 1.5934 - val_accuracy: 0.5090\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.0044 - accuracy: 0.6580 - val_loss: 1.5441 - val_accuracy: 0.4978\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9920 - accuracy: 0.6622 - val_loss: 1.5395 - val_accuracy: 0.5004\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9762 - accuracy: 0.6674 - val_loss: 1.6553 - val_accuracy: 0.4978\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9661 - accuracy: 0.6710 - val_loss: 1.5517 - val_accuracy: 0.5026\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.9496 - accuracy: 0.6771 - val_loss: 1.6081 - val_accuracy: 0.5104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8cac273850>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.4646 - accuracy: 0.1116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.464646577835083, 0.11159999668598175]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
