{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting a Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our own Time Series data\n",
    "\n",
    "##### When dealing with time series (and other types of sequences such as sentences), the input features are generally represented as 3D arrays of shape [batch size, time steps, dimensionality], where dimensionality is 1 for univariate time series and more for multivariate time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    \"\"\"\n",
    "    This function creates as many time series as requested (via the batch_size argument), \n",
    "    each of length n_steps, and there is just one value per time step in\n",
    "    each series (i.e., all series are univariate). \n",
    "    The function returns a NumPy array of shape [batch size, time steps, 1], \n",
    "    where each series is the sum of two sine waves of fixed amplitudes but random frequencies and phases, \n",
    "    plus a bit of noise.\n",
    "    \"\"\"\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1 \n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2 \n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise \n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating train/test/valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1] \n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.03852015],\n",
       "         [ 0.12723805],\n",
       "         [ 0.2254296 ],\n",
       "         ...,\n",
       "         [ 0.08864839],\n",
       "         [ 0.23562351],\n",
       "         [ 0.42870605]],\n",
       " \n",
       "        [[ 0.41015875],\n",
       "         [ 0.5966952 ],\n",
       "         [ 0.5837619 ],\n",
       "         ...,\n",
       "         [ 0.49573314],\n",
       "         [ 0.40426284],\n",
       "         [ 0.38438833]],\n",
       " \n",
       "        [[ 0.12796116],\n",
       "         [-0.02429285],\n",
       "         [-0.2951309 ],\n",
       "         ...,\n",
       "         [ 0.27147624],\n",
       "         [ 0.30975848],\n",
       "         [ 0.230126  ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.1812984 ],\n",
       "         [-0.29633474],\n",
       "         [-0.40058154],\n",
       "         ...,\n",
       "         [ 0.62124664],\n",
       "         [ 0.39424926],\n",
       "         [ 0.09833404]],\n",
       " \n",
       "        [[-0.29095381],\n",
       "         [-0.47718957],\n",
       "         [-0.56808585],\n",
       "         ...,\n",
       "         [ 0.28008774],\n",
       "         [ 0.19765839],\n",
       "         [ 0.18636045]],\n",
       " \n",
       "        [[ 0.52851313],\n",
       "         [ 0.50694364],\n",
       "         [ 0.36168638],\n",
       "         ...,\n",
       "         [ 0.18446174],\n",
       "         [ 0.31761727],\n",
       "         [ 0.3488113 ]]], dtype=float32), (7000, 50, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected network. \n",
    "\n",
    "Since it expects a flat list of features for each input, we need to add a Flatten layer. Let’s just use\n",
    "a simple Linear Regression model so that each prediction will be a linear combination of the values in the time series:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_funny = keras.models.Sequential([ \n",
    "    keras.layers.Flatten(input_shape=[50, 1]), \n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_funny.compile(loss='mse', optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "7000/7000 [==============================] - 0s 60us/sample - loss: 0.1005 - accuracy: 0.0000e+00 - val_loss: 0.0448 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "7000/7000 [==============================] - 0s 28us/sample - loss: 0.0301 - accuracy: 0.0000e+00 - val_loss: 0.0217 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "7000/7000 [==============================] - 0s 28us/sample - loss: 0.0166 - accuracy: 0.0000e+00 - val_loss: 0.0133 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "7000/7000 [==============================] - 0s 28us/sample - loss: 0.0120 - accuracy: 0.0000e+00 - val_loss: 0.0104 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "7000/7000 [==============================] - 0s 30us/sample - loss: 0.0101 - accuracy: 0.0000e+00 - val_loss: 0.0091 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "7000/7000 [==============================] - 0s 28us/sample - loss: 0.0091 - accuracy: 0.0000e+00 - val_loss: 0.0082 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "7000/7000 [==============================] - 0s 28us/sample - loss: 0.0082 - accuracy: 0.0000e+00 - val_loss: 0.0074 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "7000/7000 [==============================] - 0s 28us/sample - loss: 0.0074 - accuracy: 0.0000e+00 - val_loss: 0.0068 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "7000/7000 [==============================] - 0s 28us/sample - loss: 0.0068 - accuracy: 0.0000e+00 - val_loss: 0.0063 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "7000/7000 [==============================] - 0s 28us/sample - loss: 0.0063 - accuracy: 0.0000e+00 - val_loss: 0.0058 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe2cd066f90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_funny.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 20us/sample - loss: 0.0058 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.005755842175334692, 0.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_funny.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a single RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "7000/7000 [==============================] - 2s 313us/sample - loss: 0.1615 - val_loss: 0.1254\n",
      "Epoch 2/20\n",
      "7000/7000 [==============================] - 2s 232us/sample - loss: 0.1107 - val_loss: 0.0952\n",
      "Epoch 3/20\n",
      "7000/7000 [==============================] - 2s 231us/sample - loss: 0.0830 - val_loss: 0.0719\n",
      "Epoch 4/20\n",
      "7000/7000 [==============================] - 2s 232us/sample - loss: 0.0641 - val_loss: 0.0572\n",
      "Epoch 5/20\n",
      "7000/7000 [==============================] - 2s 233us/sample - loss: 0.0521 - val_loss: 0.0475\n",
      "Epoch 6/20\n",
      "7000/7000 [==============================] - 2s 232us/sample - loss: 0.0439 - val_loss: 0.0406\n",
      "Epoch 7/20\n",
      "7000/7000 [==============================] - 2s 236us/sample - loss: 0.0380 - val_loss: 0.0355\n",
      "Epoch 8/20\n",
      "7000/7000 [==============================] - 2s 245us/sample - loss: 0.0334 - val_loss: 0.0315\n",
      "Epoch 9/20\n",
      "7000/7000 [==============================] - 2s 235us/sample - loss: 0.0299 - val_loss: 0.0283\n",
      "Epoch 10/20\n",
      "7000/7000 [==============================] - 2s 230us/sample - loss: 0.0270 - val_loss: 0.0257\n",
      "Epoch 11/20\n",
      "7000/7000 [==============================] - 2s 235us/sample - loss: 0.0246 - val_loss: 0.0235\n",
      "Epoch 12/20\n",
      "7000/7000 [==============================] - 2s 234us/sample - loss: 0.0225 - val_loss: 0.0216\n",
      "Epoch 13/20\n",
      "7000/7000 [==============================] - 2s 243us/sample - loss: 0.0208 - val_loss: 0.0200\n",
      "Epoch 14/20\n",
      "7000/7000 [==============================] - 2s 235us/sample - loss: 0.0193 - val_loss: 0.0186\n",
      "Epoch 15/20\n",
      "7000/7000 [==============================] - 2s 248us/sample - loss: 0.0181 - val_loss: 0.0175\n",
      "Epoch 16/20\n",
      "7000/7000 [==============================] - 2s 230us/sample - loss: 0.0169 - val_loss: 0.0164\n",
      "Epoch 17/20\n",
      "7000/7000 [==============================] - 2s 231us/sample - loss: 0.0160 - val_loss: 0.0155\n",
      "Epoch 18/20\n",
      "7000/7000 [==============================] - 2s 231us/sample - loss: 0.0151 - val_loss: 0.0147\n",
      "Epoch 19/20\n",
      "7000/7000 [==============================] - 2s 231us/sample - loss: 0.0144 - val_loss: 0.0140\n",
      "Epoch 20/20\n",
      "7000/7000 [==============================] - 2s 230us/sample - loss: 0.0137 - val_loss: 0.0134\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([ \n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer='adam')\n",
    "history = model.fit(X_train, y_train, epochs=20,validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the SimpleRNN layer uses the hyperbolic tangent activation function. It works exactly as we saw earlier: the initial state h(init) is set to 0, and it is passed to a single recurrent neuron, along with the value of the first time step, x(0). The neuron computes a weighted sum of these values and applies the hyperbolic tangent activation function to the result, and this gives the first output, y0. In a simple RNN, this output is also the new state h0. This new state is passed to the same recurrent neuron along with the next input value, x(1), and the process is repeated until the last time step. Then the layer just outputs the last value, y49. All of this is performed simultaneously for every time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 71us/sample - loss: 0.0134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.013431698046624661"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep RNN\n",
    "\n",
    "##### By default, recurrent layers in Keras only return the final output. To make them return one output per time step, you must set return_sequences=True, as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "7000/7000 [==============================] - 6s 899us/sample - loss: 0.0271 - val_loss: 0.0049\n",
      "Epoch 2/20\n",
      "7000/7000 [==============================] - 5s 700us/sample - loss: 0.0043 - val_loss: 0.0037\n",
      "Epoch 3/20\n",
      "7000/7000 [==============================] - 5s 760us/sample - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 4/20\n",
      "7000/7000 [==============================] - 5s 697us/sample - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 5/20\n",
      "7000/7000 [==============================] - 5s 701us/sample - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 6/20\n",
      "7000/7000 [==============================] - 5s 676us/sample - loss: 0.0032 - val_loss: 0.0032\n",
      "Epoch 7/20\n",
      "7000/7000 [==============================] - 5s 671us/sample - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 8/20\n",
      "7000/7000 [==============================] - 5s 669us/sample - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 9/20\n",
      "7000/7000 [==============================] - 5s 668us/sample - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 10/20\n",
      "7000/7000 [==============================] - 5s 666us/sample - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 11/20\n",
      "7000/7000 [==============================] - 5s 667us/sample - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 12/20\n",
      "7000/7000 [==============================] - 5s 669us/sample - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 13/20\n",
      "7000/7000 [==============================] - 5s 668us/sample - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 14/20\n",
      "7000/7000 [==============================] - 5s 659us/sample - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 15/20\n",
      "7000/7000 [==============================] - 5s 660us/sample - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 16/20\n",
      "7000/7000 [==============================] - 5s 661us/sample - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 17/20\n",
      "7000/7000 [==============================] - 5s 657us/sample - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 18/20\n",
      "7000/7000 [==============================] - 5s 660us/sample - loss: 0.0029 - val_loss: 0.0032\n",
      "Epoch 19/20\n",
      "7000/7000 [==============================] - 5s 669us/sample - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 20/20\n",
      "7000/7000 [==============================] - 5s 662us/sample - loss: 0.0028 - val_loss: 0.0028\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING!\n",
    "Make sure to set return_sequences=True for all recurrent layers (except the last one, if you only care about the last output). If you don’t, they will output a 2D array (containing only the output of the last time step) instead of a 3D array (containing outputs for all time steps), and the next recurrent layer will complain that you are not feeding it sequences in the expected 3D format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 171us/sample - loss: 0.0028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0027888535149395467"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleRNN layer uses the tanh activation function by default, the predicted values must lie within the range –1 to 1. But what if you want to use another activation function? For both these reasons, it might be preferable to replace the output layer with a Dense layer: it would run slightly faster, the accuracy would be roughly the same, and it would allow us to choose any output activation function we want. If you make this change, also make sure to remove return_sequences=True from the second (now last) recurrent layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "7000/7000 [==============================] - 5s 645us/sample - loss: 0.0209 - val_loss: 0.0049\n",
      "Epoch 2/20\n",
      "7000/7000 [==============================] - 4s 505us/sample - loss: 0.0042 - val_loss: 0.0039\n",
      "Epoch 3/20\n",
      "7000/7000 [==============================] - 3s 480us/sample - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 4/20\n",
      "7000/7000 [==============================] - 3s 476us/sample - loss: 0.0033 - val_loss: 0.0035\n",
      "Epoch 5/20\n",
      "7000/7000 [==============================] - 3s 477us/sample - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 6/20\n",
      "7000/7000 [==============================] - 3s 500us/sample - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 7/20\n",
      "7000/7000 [==============================] - 3s 487us/sample - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 8/20\n",
      "7000/7000 [==============================] - 4s 511us/sample - loss: 0.0031 - val_loss: 0.0032\n",
      "Epoch 9/20\n",
      "7000/7000 [==============================] - 3s 477us/sample - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 10/20\n",
      "7000/7000 [==============================] - 3s 483us/sample - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 11/20\n",
      "7000/7000 [==============================] - 3s 500us/sample - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 12/20\n",
      "7000/7000 [==============================] - 3s 482us/sample - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 13/20\n",
      "7000/7000 [==============================] - 4s 505us/sample - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 14/20\n",
      "7000/7000 [==============================] - 4s 522us/sample - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 15/20\n",
      "7000/7000 [==============================] - 4s 513us/sample - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 16/20\n",
      "7000/7000 [==============================] - 4s 518us/sample - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 17/20\n",
      "7000/7000 [==============================] - 4s 506us/sample - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 18/20\n",
      "7000/7000 [==============================] - 4s 505us/sample - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 19/20\n",
      "7000/7000 [==============================] - 4s 505us/sample - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 20/20\n",
      "7000/7000 [==============================] - 4s 507us/sample - loss: 0.0029 - val_loss: 0.0030\n",
      "2000/2000 [==============================] - 0s 111us/sample - loss: 0.0030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.002951365625485778"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), \n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
